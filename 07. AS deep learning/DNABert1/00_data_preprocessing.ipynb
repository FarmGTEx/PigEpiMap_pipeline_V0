{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import os\n",
    "\n",
    "def kmer2seq(kmers):\n",
    "    \"\"\"\n",
    "    Convert kmers to original sequence\n",
    "    \n",
    "    Arguments:\n",
    "    kmers -- str, kmers separated by space.\n",
    "    \n",
    "    Returns:\n",
    "    seq -- str, original sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    kmers_list = kmers.split(\" \")\n",
    "    bases = [kmer[0] for kmer in kmers_list[0:-1]]\n",
    "    bases.append(kmers_list[-1])\n",
    "    seq = \"\".join(bases)\n",
    "    assert len(seq) == len(kmers_list) + len(kmers_list[0]) - 1\n",
    "    return seq\n",
    "\n",
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \n",
    "    Arguments:\n",
    "    seq -- str, original sequence.\n",
    "    k -- int, kmer of length k specified.\n",
    "    \n",
    "    Returns:\n",
    "    kmers -- str, kmers separated by space\n",
    "\n",
    "    \"\"\"\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers\n",
    "\n",
    "\n",
    "def get_train_valid_idx(sample_num):\n",
    "    \"\"\"\n",
    "    Obtain the indices for the training set and validation set according to an 8:2 ratio\n",
    "    \"\"\"\n",
    "    train_idx = []\n",
    "    valid_idx = []\n",
    "    random_idx = list(range(sample_num))\n",
    "    random.shuffle(random_idx)\n",
    "    avg_group_count = 10\n",
    "    step = int(len(random_idx)/avg_group_count)\n",
    "    random_idx_group = [random_idx[i:i+step] for i in range(0,len(random_idx),step)]\n",
    "    for i in range(8):\n",
    "        train_idx.extend(random_idx_group[i])\n",
    "    for i in range(8,len(random_idx_group)):\n",
    "        valid_idx.extend(random_idx_group[i])\n",
    "    return {'train':train_idx,'valid':valid_idx}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train set and validation set\n",
    "\n",
    "Generate and save the training and validation datasets to the \"train_valid_csv\" directory based on the \"fa_diff_len\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fasta_dir = \"./fa_diff_len/\"\n",
    "train_data_dir = \"./train_valid_tsv\"\n",
    "os.makedirs(train_data_dir,exist_ok=True)\n",
    "for specie in ['human','pig']:\n",
    "    if specie=='pig':\n",
    "        extend_lens = [64,128,256]\n",
    "        cre_types = ['e1','e4e5','e10','e1_e4e5_e10']\n",
    "    elif specie==\"human\":\n",
    "        extend_lens = [64,128,175,256]\n",
    "        cre_types = ['promoter','enhancer','promoter_enhancer']\n",
    "    for extend_len in extend_lens:\n",
    "        for cre_type in cre_types:\n",
    "            train_dir = f\"{cre_type}_{extend_len*2+1}\"\n",
    "            absolute_dir = os.path.join(train_data_dir,specie,train_dir)\n",
    "            os.mkdir(absolute_dir)\n",
    "            for train_valid in ['train','valid']:\n",
    "                output_tsv = \"train.tsv\" if train_valid==\"train\" else \"dev.tsv\"\n",
    "                tsv_path = os.path.join(absolute_dir,output_tsv)\n",
    "                seqs = {}\n",
    "                kmers_label = {}\n",
    "                for pos_neg in ['positive','negative']:\n",
    "                    seq_label = 1 if pos_neg==\"positive\" else 0\n",
    "                    fafile = os.path.join(fasta_dir,specie,f\"{cre_type}_len_{extend_len*2+1}_{train_valid}_{pos_neg}.fa\")\n",
    "                    fa = pysam.FastaFile(fafile)\n",
    "                    seqs[pos_neg] = [fa.fetch(rederence) for rederence in fa.references]\n",
    "                    kmers_label[pos_neg] = [(seq2kmer(seq,k=6),seq_label) for seq in seqs[pos_neg]]\n",
    "                merge_sample = kmers_label['positive']+kmers_label['negative']\n",
    "                random.shuffle(merge_sample)\n",
    "                df_tsv = pd.DataFrame(merge_sample)\n",
    "                df_tsv.columns = ['sequence','label']\n",
    "                df_tsv.to_csv(tsv_path,sep=\"\\t\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate model fine-tuning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmd(data_path,output_path,cuda_divice,seq_len,loggin_steps,save_steps,batch_size,n_process=32):\n",
    "\n",
    "    bashFile = f\"\"\"\n",
    "\n",
    "    export KMER=6\n",
    "    export MODEL_PATH=/data2/zyd_workspace/2024_10_pig_AS_SNV/DNABERT/6-new-12w-0\n",
    "    export DATA_PATH={data_path}\n",
    "    export OUTPUT_PATH={output_path}\n",
    "\n",
    "    CUDA_VISIBLE_DEVICES={cuda_divice} python run_finetune.py --model_type dna --tokenizer_name=dna$KMER --model_name_or_path $MODEL_PATH --task_name dnaprom --do_train --do_eval --data_dir $DATA_PATH --max_seq_length {seq_len} --per_gpu_eval_batch_size={batch_size}   --per_gpu_train_batch_size={batch_size}   --learning_rate 5e-5 --num_train_epochs 20 --output_dir $OUTPUT_PATH --evaluate_during_training --logging_steps {loggin_steps} --save_steps {save_steps} --warmup_percent 0.1 --hidden_dropout_prob 0.1 --overwrite_output --weight_decay 0.01 --n_process {n_process}\n",
    "\n",
    "    \"\"\"\n",
    "    return bashFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = \"./train_valid_tsv\"\n",
    "train_output_dir = \"./train_output\"\n",
    "os.makedirs(train_output_dir,exist_ok=True)\n",
    "all_train_cmd = []\n",
    "for specie in ['human','pig']:\n",
    "    if specie=='pig':\n",
    "        extend_lens = [256,128,64]\n",
    "        cre_types = ['e1','e4e5','e10','e1_e4e5_e10']\n",
    "    elif specie==\"human\":\n",
    "        extend_lens = [256,175,128,64]\n",
    "        cre_types = ['promoter','enhancer','promoter_enhancer']\n",
    "    for extend_len in extend_lens:\n",
    "        for cre_type in cre_types:\n",
    "            train_dir = f\"{cre_type}_{extend_len*2+1}\"\n",
    "            data_path = os.path.join(train_data_dir,specie,train_dir)\n",
    "            output_path = os.path.join(train_output_dir,specie,train_dir)\n",
    "            df_train_tsv = pd.read_csv(os.path.join(data_path,\"train.tsv\"),sep=\"\\t\")\n",
    "            loggin_steps = int(df_train_tsv.shape[0]/batch_size)\n",
    "            save_steps = int(df_train_tsv.shape[0]/batch_size)\n",
    "            seq_len = extend_len*2+1-6+1+2\n",
    "            cuda_divice = 1 if extend_len<175 else 0\n",
    "            all_train_cmd.append(get_cmd(data_path,output_path,cuda_divice,seq_len,loggin_steps,save_steps,batch_size,n_process=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_4090_1 = open(\"./train_4090_1.sh\",\"w\")\n",
    "f_4090_2 = open(\"./train_4090_2.sh\",\"w\")\n",
    "for i in all_train_cmd:\n",
    "    if \"CUDA_VISIBLE_DEVICES=0\" in i:\n",
    "        f_4090_1.write(f\"{i}\\n\")\n",
    "    else:\n",
    "        f_4090_2.write(f\"{i}\\n\")\n",
    "f_4090_1.close()\n",
    "f_4090_2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
